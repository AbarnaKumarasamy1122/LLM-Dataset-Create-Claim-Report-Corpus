{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e5e358a",
   "metadata": {},
   "source": [
    "## 1ï¸âƒ£ Environment Setup\n",
    "\n",
    "Choose one option below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e75eb75",
   "metadata": {},
   "source": [
    "### Option A: Upload Project Folder (Recommended for small projects)\n",
    "\n",
    "1. Click the **folder icon** ðŸ“ on the left sidebar\n",
    "2. Click **Upload** button\n",
    "3. Upload your entire `llm_dataset` folder (or zip it first)\n",
    "4. Skip to **Step 2: Verify GPU**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c46e98b",
   "metadata": {},
   "source": [
    "### Option B: Mount Google Drive (Recommended for large projects)\n",
    "\n",
    "Run the cell below to mount your Google Drive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c81763",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Change to your project directory\n",
    "\n",
    "# Update this path to match your Google Drive structure\n",
    "%cd /content/drive/MyDrive/llm_dataset\n",
    "\n",
    "print(\"âœ… Google Drive mounted successfully!\")\n",
    "print(f\"ðŸ“ Current directory: {!pwd}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6588401e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "89359a57",
   "metadata": {},
   "source": [
    "## 2ï¸âƒ£ Verify GPU is Available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b768d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(\"ðŸ–¥ï¸  GPU Check:\")\n",
    "print(f\"   CUDA Available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    print(\"   âœ… GPU is ready for training!\")\n",
    "else:\n",
    "    print(\"   âš ï¸  GPU not detected!\")\n",
    "    print(\"   Go to: Runtime â†’ Change runtime type â†’ Hardware accelerator â†’ GPU\")\n",
    "\n",
    "# Check PyTorch version\n",
    "print(f\"\\nðŸ“¦ PyTorch Version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b383f1",
   "metadata": {},
   "source": [
    "## 3ï¸âƒ£ Install Required Packages\n",
    "\n",
    "This will install all dependencies (takes ~5 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21e1e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Install dependencies (suppress output to reduce clutter)\n",
    "!pip install transformers>=4.35.0 datasets>=2.14.0 peft>=0.7.0 accelerate>=0.24.0\n",
    "!pip install evaluate>=0.4.0 rouge-score>=0.1.2 sentencepiece>=0.1.99 protobuf>=3.20.0\n",
    "\n",
    "print(\"âœ… All packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1cd8001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify installations\n",
    "import transformers\n",
    "import datasets\n",
    "import peft\n",
    "import accelerate\n",
    "import evaluate\n",
    "\n",
    "print(\"ðŸ“¦ Package Versions:\")\n",
    "print(f\"   transformers: {transformers.__version__}\")\n",
    "print(f\"   datasets: {datasets.__version__}\")\n",
    "print(f\"   peft: {peft.__version__}\")\n",
    "print(f\"   accelerate: {accelerate.__version__}\")\n",
    "print(\"\\nâœ… All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd8bc74",
   "metadata": {},
   "source": [
    "## 4ï¸âƒ£ Verify Dataset Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4f91f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# Check required files\n",
    "required_files = [\n",
    "    'model_training/clean_train.jsonl',\n",
    "    'model_training/clean_val.jsonl',\n",
    "    'model_training/clean_test.jsonl',\n",
    "    'scripts/train_model.py'\n",
    "]\n",
    "\n",
    "print(\"ðŸ“Š Dataset Verification:\")\n",
    "all_present = True\n",
    "\n",
    "for file in required_files:\n",
    "    if os.path.exists(file):\n",
    "        size = os.path.getsize(file) / (1024 * 1024)  # MB\n",
    "        \n",
    "        # Count lines for JSONL files\n",
    "        if file.endswith('.jsonl'):\n",
    "            with open(file, 'r', encoding='utf-8') as f:\n",
    "                lines = sum(1 for _ in f)\n",
    "            print(f\"   âœ… {file} ({size:.2f} MB, {lines} examples)\")\n",
    "        else:\n",
    "            print(f\"   âœ… {file} ({size:.2f} MB)\")\n",
    "    else:\n",
    "        print(f\"   âŒ {file} - MISSING\")\n",
    "        all_present = False\n",
    "\n",
    "if all_present:\n",
    "    print(\"\\nâœ… All required files present!\")\n",
    "    \n",
    "    # Show sample data\n",
    "    print(\"\\nðŸ“ Sample Training Data:\")\n",
    "    with open('model_training/clean_train.jsonl', 'r', encoding='utf-8') as f:\n",
    "        sample = json.loads(f.readline())\n",
    "        print(f\"   Input length: {len(sample['input'])} chars\")\n",
    "        print(f\"   Output length: {len(sample['output'])} chars\")\n",
    "        print(f\"   Input preview: {sample['input'][:100]}...\")\n",
    "else:\n",
    "    print(\"\\nâŒ Some files are missing. Please upload your project folder.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ea2d6b",
   "metadata": {},
   "source": [
    "## 5ï¸âƒ£ Start Model Training\n",
    "\n",
    "This will train FLAN-T5-base with LoRA for 3 epochs (~2-4 hours)\n",
    "\n",
    "**What happens:**\n",
    "- Downloads FLAN-T5-base model from Hugging Face\n",
    "- Applies LoRA adapters (99% memory reduction)\n",
    "- Trains for 3 epochs with validation\n",
    "- Saves best checkpoint\n",
    "- Runs test evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f482f5d",
   "metadata": {},
   "source": [
    "## ðŸ§¹ Clear GPU Memory (Run This Before Training!)\n",
    "\n",
    "**IMPORTANT:** Run this cell to clear any existing GPU memory before starting training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203b1395",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "\n",
    "print(\"ðŸ§¹ Clearing GPU memory...\")\n",
    "\n",
    "# Clear Python garbage collector\n",
    "gc.collect()\n",
    "\n",
    "# Clear CUDA cache\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    \n",
    "    print(f\"âœ… GPU memory cleared!\")\n",
    "    print(f\"   Allocated: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB\")\n",
    "    print(f\"   Reserved: {torch.cuda.memory_reserved(0) / 1e9:.2f} GB\")\n",
    "    print(f\"   Free: {(torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_allocated(0)) / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"âš ï¸  No GPU available\")\n",
    "\n",
    "print(\"\\nðŸ’¡ Memory optimization tips:\")\n",
    "print(\"   â€¢ If training fails with OOM, restart runtime: Runtime â†’ Restart runtime\")\n",
    "print(\"   â€¢ Close other tabs/programs using GPU\")\n",
    "print(\"   â€¢ The script now uses gradient accumulation (effective batch size = 4)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cdeb6f8",
   "metadata": {},
   "source": [
    "## ðŸš€ Start Training\n",
    "\n",
    "**What's been optimized for T4 GPU (14.74 GB):**\n",
    "- âœ… Batch size: 1 (per device)\n",
    "- âœ… Gradient accumulation: 4 steps (effective batch = 4)\n",
    "- âœ… Evaluation batch size: 1\n",
    "- âœ… Max input tokens: 100 (reduced from 128)\n",
    "- âœ… Max output tokens: 200 (reduced from 256)\n",
    "- âœ… Memory-efficient evaluation (prediction_loss_only=True)\n",
    "- âœ… FP16 mixed precision enabled\n",
    "- âœ… Validation metric: eval_loss (ROUGE computed after training)\n",
    "\n",
    "**Note:** During training, only validation loss is tracked. Full BLEU/ROUGE evaluation happens after training completes.\n",
    "\n",
    "This will take ~2-4 hours. Training evaluates at the end of each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b14a794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the training script\n",
    "!python scripts/train_model.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e39419b",
   "metadata": {},
   "source": [
    "### ðŸ“Š Monitor Training Progress\n",
    "\n",
    "While training runs, you'll see:\n",
    "- **Epoch progress bars** (1/3, 2/3, 3/3)\n",
    "- **Training loss** (should decrease over time)\n",
    "- **Validation loss** (eval_loss) at end of each epoch\n",
    "- **GPU memory usage**\n",
    "\n",
    "**What you WON'T see during training:**\n",
    "- ROUGE scores (computed after training in Step 7)\n",
    "- BLEU scores (computed after training in Step 7)\n",
    "\n",
    "**Expected Timeline:**\n",
    "- Epoch 1: ~17-20 minutes training + 1 min validation\n",
    "- Epoch 2: ~17-20 minutes training + 1 min validation  \n",
    "- Epoch 3: ~17-20 minutes training + 1 min validation\n",
    "- Total: ~60-65 minutes (about 1 hour)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002f3158",
   "metadata": {},
   "source": [
    "## 6ï¸âƒ£ Check Training Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4323bbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Check if training completed\n",
    "if os.path.exists('model_training/fine_tuned_model/test_results.json'):\n",
    "    print(\"âœ… Training completed successfully!\\n\")\n",
    "    \n",
    "    # Load test results\n",
    "    with open('model_training/fine_tuned_model/test_results.json', 'r') as f:\n",
    "        results = json.load(f)\n",
    "    \n",
    "    print(\"ðŸ“Š Test Set Results:\")\n",
    "    \n",
    "    # Safely format results (handle both numeric and 'N/A' values)\n",
    "    eval_loss = results.get('eval_loss', 'N/A')\n",
    "    if isinstance(eval_loss, (int, float)):\n",
    "        print(f\"   Loss: {eval_loss:.4f}\")\n",
    "    else:\n",
    "        print(f\"   Loss: {eval_loss}\")\n",
    "    \n",
    "    # Note: ROUGE scores not computed during training (prediction_loss_only=True)\n",
    "    # They will be computed in Step 7 via evaluate_model.py\n",
    "    print(\"\\nðŸ’¡ Note: ROUGE/BLEU scores not computed during training.\")\n",
    "    print(\"   Run Step 7 (evaluate_model.py) for comprehensive metrics.\")\n",
    "    \n",
    "    # Check model files\n",
    "    print(\"\\nðŸ“ Model Files:\")\n",
    "    model_dir = 'model_training/fine_tuned_model'\n",
    "    if os.path.exists(model_dir):\n",
    "        for file in os.listdir(model_dir):\n",
    "            file_path = os.path.join(model_dir, file)\n",
    "            if os.path.isfile(file_path):\n",
    "                size = os.path.getsize(file_path) / (1024 * 1024)\n",
    "                print(f\"   {file} ({size:.2f} MB)\")\n",
    "    else:\n",
    "        print(\"   Model directory not found\")\n",
    "else:\n",
    "    print(\"â³ Training not yet complete or failed.\")\n",
    "    print(\"   Check the output above for errors.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df4ffc7",
   "metadata": {},
   "source": [
    "## 7ï¸âƒ£ Run Comprehensive Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edcd50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation script\n",
    "!python scripts/evaluate_model.py \\\n",
    "  --model_path model_training/fine_tuned_model \\\n",
    "  --test_data model_training/clean_test.jsonl \\\n",
    "  --output model_training/evaluation_report.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73600c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View evaluation summary\n",
    "import pandas as pd\n",
    "\n",
    "if os.path.exists('model_training/evaluation_report.csv'):\n",
    "    df = pd.read_csv('model_training/evaluation_report.csv')\n",
    "    \n",
    "    print(\"ðŸ“Š Evaluation Summary:\")\n",
    "    print(f\"   Total examples: {len(df)}\")\n",
    "    \n",
    "    # Show available columns\n",
    "    print(f\"\\nðŸ“‹ Available columns: {', '.join(df.columns.tolist())}\")\n",
    "    \n",
    "    # Try to display metrics with error handling\n",
    "    print(\"\\nðŸ“ˆ Metrics:\")\n",
    "    \n",
    "    # Check and display BLEU scores\n",
    "    bleu_cols = [col for col in df.columns if col.startswith('bleu')]\n",
    "    if bleu_cols:\n",
    "        for col in bleu_cols:\n",
    "            print(f\"   Average {col.upper()}: {df[col].mean():.4f}\")\n",
    "    else:\n",
    "        print(\"   âš ï¸  BLEU scores not found in report\")\n",
    "    \n",
    "    # Check and display ROUGE scores\n",
    "    rouge_cols = [col for col in df.columns if 'rouge' in col.lower()]\n",
    "    if rouge_cols:\n",
    "        for col in rouge_cols:\n",
    "            print(f\"   Average {col}: {df[col].mean():.4f}\")\n",
    "    else:\n",
    "        print(\"   âš ï¸  ROUGE scores not found in report\")\n",
    "    \n",
    "    # Check for metadata consistency\n",
    "    if 'metadata_consistency' in df.columns:\n",
    "        print(f\"   Metadata Consistency: {df['metadata_consistency'].mean():.2%}\")\n",
    "    \n",
    "    # Show top 5 if we have rouge_l column\n",
    "    if 'rouge_l' in df.columns:\n",
    "        print(\"\\nðŸŒŸ Top 5 Best Predictions (by ROUGE-L):\")\n",
    "        display_cols = ['rouge_l'] + [c for c in ['bleu_4', 'bleu_1'] if c in df.columns]\n",
    "        top5 = df.nlargest(5, 'rouge_l')[display_cols]\n",
    "        print(top5.to_string(index=False))\n",
    "        \n",
    "        # Show distribution\n",
    "        print(\"\\nðŸ“ˆ ROUGE-L Distribution:\")\n",
    "        print(df['rouge_l'].describe())\n",
    "    \n",
    "    # Show first few rows for inspection\n",
    "    print(\"\\nðŸ“„ Sample of evaluation data:\")\n",
    "    print(df.head(3).to_string())\n",
    "else:\n",
    "    print(\"âŒ Evaluation report not found.\")\n",
    "    print(\"   Make sure to run the previous cell to generate the evaluation report.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7f6ac6",
   "metadata": {},
   "source": [
    "## 8ï¸âƒ£ Test Inference Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a91c092",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with example file\n",
    "!python scripts/claim_report_generation.py \\\n",
    "  --model_path model_training/fine_tuned_model \\\n",
    "  --input model_training/example_input.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd1f86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive test - Load model and generate\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import json\n",
    "\n",
    "print(\"ðŸš€ Loading model for interactive testing...\")\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_path = 'model_training/fine_tuned_model'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.to('cuda')\n",
    "    print(\"âœ… Model loaded on GPU\\n\")\n",
    "else:\n",
    "    print(\"âœ… Model loaded on CPU\\n\")\n",
    "\n",
    "# Test samples\n",
    "test_samples = [\n",
    "    {\n",
    "        \"shipment_id\": \"SH12345\",\n",
    "        \"product_id\": \"PRD789\",\n",
    "        \"damage_type\": \"dent\",\n",
    "        \"damage_severity\": \"moderate\",\n",
    "        \"damage_location\": \"corner\",\n",
    "        \"carrier\": \"FastShip Logistics\",\n",
    "        \"inspection_date\": \"2025-11-27\"\n",
    "    },\n",
    "    {\n",
    "        \"shipment_id\": \"SH67890\",\n",
    "        \"product_id\": \"PRD456\",\n",
    "        \"damage_type\": \"wet\",\n",
    "        \"damage_severity\": \"severe\",\n",
    "        \"damage_location\": \"entire package\",\n",
    "        \"carrier\": \"QuickMove Express\",\n",
    "        \"inspection_date\": \"2025-11-27\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Generate reports\n",
    "for i, sample in enumerate(test_samples, 1):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Test Case {i}: {sample['damage_type'].upper()} damage\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Create input text\n",
    "    input_text = json.dumps(sample)\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        inputs = {k: v.to('cuda') for k, v in inputs.items()}\n",
    "    \n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_length=512,\n",
    "            num_beams=4,\n",
    "            temperature=0.7,\n",
    "            do_sample=False\n",
    "        )\n",
    "    \n",
    "    # Decode\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    print(f\"\\nðŸ“ Generated Claim Report:\\n\")\n",
    "    print(generated_text)\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"âœ… Inference testing complete!\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52d4630",
   "metadata": {},
   "source": [
    "## 9ï¸âƒ£ Download Results\n",
    "\n",
    "Download trained model and results to your computer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc6e859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a zip file of results\n",
    "import shutil\n",
    "\n",
    "print(\"ðŸ“¦ Creating zip file of results...\")\n",
    "\n",
    "# Create zip of fine_tuned_model\n",
    "if os.path.exists('model_training/fine_tuned_model'):\n",
    "    shutil.make_archive('fine_tuned_model', 'zip', 'model_training/fine_tuned_model')\n",
    "    print(\"   âœ… fine_tuned_model.zip created\")\n",
    "\n",
    "# Create zip of evaluation results\n",
    "files_to_zip = [\n",
    "    'model_training/evaluation_report.csv',\n",
    "    'model_training/evaluation_samples.txt',\n",
    "    'model_training/fine_tuned_model/test_results.json'\n",
    "]\n",
    "\n",
    "print(\"\\nðŸ“¥ Files ready for download:\")\n",
    "print(\"   1. fine_tuned_model.zip (trained model)\")\n",
    "print(\"   2. evaluation_report.csv (metrics)\")\n",
    "print(\"   3. evaluation_samples.txt (sample predictions)\")\n",
    "print(\"\\nðŸ’¡ Use the Files panel (ðŸ“) on the left to download these files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52bbb830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Copy results to Google Drive\n",
    "if os.path.exists('/content/drive'):\n",
    "    print(\"ðŸ’¾ Copying results to Google Drive...\")\n",
    "    \n",
    "    # Create output directory in Drive\n",
    "    drive_output = '/content/drive/MyDrive/Week5_Results'\n",
    "    os.makedirs(drive_output, exist_ok=True)\n",
    "    \n",
    "    # Copy key files\n",
    "    if os.path.exists('fine_tuned_model.zip'):\n",
    "        shutil.copy('fine_tuned_model.zip', drive_output)\n",
    "        print(\"   âœ… Model copied to Drive\")\n",
    "    \n",
    "    if os.path.exists('model_training/evaluation_report.csv'):\n",
    "        shutil.copy('model_training/evaluation_report.csv', drive_output)\n",
    "        print(\"   âœ… Evaluation report copied to Drive\")\n",
    "    \n",
    "    print(f\"\\nâœ… Results saved to: {drive_output}\")\n",
    "else:\n",
    "    print(\"â„¹ï¸  Google Drive not mounted. Files available in Colab session only.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f3a536",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Summary\n",
    "\n",
    "### Completed Tasks:\n",
    "- âœ… Environment setup (GPU verified)\n",
    "- âœ… Dependencies installed\n",
    "- âœ… Model training (FLAN-T5-base with LoRA)\n",
    "- âœ… Comprehensive evaluation\n",
    "- âœ… Inference testing\n",
    "- âœ… Results ready for download\n",
    "\n",
    "### Next Steps:\n",
    "1. **Download Results:** Use Files panel to download `fine_tuned_model.zip` and `evaluation_report.csv`\n",
    "2. **Review Metrics:** Check ROUGE-L and BLEU scores in evaluation report\n",
    "3. **Test Locally:** Use the trained model with `claim_report_generation.py`\n",
    "4. **Deploy API (Optional):** Run `app.py` for FastAPI deployment\n",
    "5. **Create Demo Video:** Record inference examples\n",
    "\n",
    "### Key Deliverables:\n",
    "- âœ… `fine_tuned_model/` - Trained model weights\n",
    "- âœ… `evaluation_report.csv` - Detailed metrics\n",
    "- âœ… `claim_report_generation.py` - Inference pipeline\n",
    "- âœ… Test results and sample predictions\n",
    "\n",
    "---\n",
    "\n",
    "**ðŸŽ‰ Week 5 Training Complete!**\n",
    "\n",
    "**Training Time:** Check the output above for actual duration\n",
    "\n",
    "**Quality:** Review ROUGE-L score (target: >0.50)\n",
    "\n",
    "**Status:** Ready for deployment and demo video creation\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
